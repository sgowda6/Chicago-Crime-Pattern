# -*- coding: utf-8 -*-
"""IDS561_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvSH9tdFl9m8QYS8ru0e6VtJXPX46u0O
"""

from google.colab import drive
drive.mount('/content/drive')
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://apache.mirrors.pair.com/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

!tar -xvf spark-3.0.1-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"

#Create spark session

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

#import libraries
import pandas as pd
import re
from pyspark import SparkContext
import datetime
from pyspark.sql.functions import *
from pyspark.sql.types import IntegerType
from pyspark import SparkContext
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType

#read CSV into a DF
df = spark.read.format('csv').options(header='true').load('Crimes_-_2013_to_Present.csv')

#display df

df.show()
df.count()

#drop all NA values

df = df.dropna()

#check count of data we're dealing with

df.count()

#df will be read again and again, so cache it

df = df.cache()

#check schema

df.printSchema()

#drop columns that we do not require for our analysis

df = df.drop("X Coordinate","Y Coordinate","Latitude","Longitude","Location","Updated On")

"""In our crimes dataframe, the column 'Community Area' is an indicator of which community area of Chicago a crime occurs in. We have manually created a data frame by collecting the area and matching it with the Community Area code from the Population and Poverty Data by Chicago Community Area (December 2017).  

http://www.actforchildren.org/wp-content/uploads/2018/01/Census-Data-by-Chicago-Community-Area-2017.pdf
"""

#manually insert community area codes and area names into Community_Area list

from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType
Community_Area = [ Row(1, "Rogers Park","Far North side"),Row(2, "West Ridge","Far North side"),Row(3, "Uptown","Far North side"),Row(4, "Lincoln Square","Far North side"),
               Row(5, "North Center","North side"),Row(6, "Lake View","North side"),Row(7, "Lincoln Park","North side"),Row(8, "Near North Side","Central"),
               Row(9, "Edison Park","Far North side"),Row(10, "Norwood Park","Far North side"),Row(11, "Jefferson Park","Far North side"),Row(12, "Forest Glen","Far North side"),
               Row(13, "North Park","Far North side"),Row(14, "Albany Park","Far North side"),Row(15, "Portage Park","Northwest side"),Row(16, "Irving Park","Northwest side"),
               Row(17, "Dunning","Northwest side"),Row(18, "Montclare","Northwest side"),Row(19, "Belmont Cragin","Northwest side"),Row(20, "Hermosa","Northwest side"),
               Row(21, "Avondale","North side"),Row(22, "Logan Square","North side"),Row(23, "Humboldt Park", "West side"),Row(24, "West Town","West side"),
               Row(25, "Austin","West side"),Row(26, "West Garfield Park","West side"),Row(27, "East Garfield Park","West side"),Row(28, "Near West Side","West side"),
               Row(29, "North Lawndale","West side"),Row(30, "South Lawndale","West side"),Row(31, "Lower West Side","West side"),Row(32, "Loop","Central"),
               Row(33, "Near South Side","Central"),Row(34, "Armour Square","South side"),Row(35, "Douglas","South side"),Row(36, "Oakland","South side"),
               Row(37, "Fuller Park","South side"),Row(38, "Grand Boulevard","South side"),Row(39, "Kenwood","South side"),Row(40, "Washington Park","South side"),
               Row(41, "Hyde Park","South side"),Row(42, "Woodlawn","South side"),Row(43, "South Shore","South side"),Row(44, "Chatham","Far Southeast side"),
               Row(45, "Avalon Park","Far Southeast side"),Row(46, "South Chicago","Far Southeast side"),Row(47, "Burnside","Far Southeast side"),Row(48, "Calumet Heights","Far Southeast side"),
               Row(49, "Roseland","Far Southeast side"),Row(50, "Pullman","Far Southeast side"),Row(51, "South Deering","Far Southeast side"),Row(52, "East Side","Far Southeast side"),
               Row(53, "West Pullman","Far Southeast side"),Row(54, "Riverdale","Far Southeast side"),Row(55, "Hegewisch","Far Southeast side"),Row(56, "Garfield Ridge","Southwest side"),
               Row(57, "Archer Heights","Southwest side"),Row(58, "Brighton Park","Southwest side"),Row(59, "McKinley Park","Southwest side"),Row(60, "Bridgeport","South side"),
               Row(61, "New City","Southwest side"),Row(62, "West Elsdon","Southwest side"),Row(63, "Gage Park","Southwest side"),Row(64, "Clearing","Southwest side"),
               Row(65, "West Lawn","Southwest side"),Row(66, "Chicago Lawn","Southwest side"),Row(67, "West Englewood","Southwest side"),Row(68, "Englewood","Southwest side"),
               Row(69, "Greater Grand Crossing","South side"),Row(70, "Ashburn","Far Southwest side"),Row(71, "Auburn Gresham","Far Southwest side"),Row(72, "Beverly","Far Southwest side"),
               Row(73, "Washington Heights","Far Southwest side"),Row(74, "Mount Greenwood","Far Southwest side"),Row(75, "Morgan Park","Far Southwest side"),Row(76, "O'Hare","Far North side"),
               Row(77, "Edgewater","Far North side")]

#Define schema for the above list and store it into a dataframe

community_schema = StructType([ StructField("Community_Area", IntegerType(), False),
                                StructField("Area_name", StringType(), True),
                          StructField("Area_side", StringType(), True)])
#create spark dataframe
communityDf = spark.createDataFrame(Community_Area, community_schema)

#view
communityDf.show(truncate=False)

#Join Community_Area dataframe with original dataframe df
df = df.join(communityDf,"Community_Area",'inner')

#Check
df

"""District variables in the crime data frame represent the number of the police district where the incident took place.
Adding the name of the police district would provide additional context required for review. There are 22 police districts, each of which has a police name and was accessed from the Data Portal of the City of Chicago.
https://www.chicago.gov/city/en/depts/cpd/dataset/police_stations.html
"""

#read the police station data into a spark dataframe

police_station = spark.read \
                 .format("csv") \
                 .option("mode","FAILFAST") \
                 .option("inferSchema", "true") \
                 .option("header", "true") \
                 .load("Police_Stations.csv")

#Join the police station df with df2
df = df.join(police_station,"District",'inner')

#check
df

#drop unnecessary columns from df2
df = df.drop("CITY","STATE","ZIP","WEBSITE","PHONE","FAX","TTY","X COORDINATE","Y COORDINATE","LATITUDE","LONGITUDE","LOCATION")

#check
df

"""**EXPLORATORY DATA ANALYSIS**

**1. Visualize top ten crimes in Chicago**
"""

#Types of Crime in Chicago
crime_types = df.groupBy('Primary_Type').count()

crime_type_counts = crime_types.orderBy('count', ascending=False)

#view top most occured crime types by number

crime_type_counts.show(truncate=False)

#convert to pandas 

crime_type_counts.toPandas()

#convert back to rdd

counts = pd.DataFrame(crime_type_counts.rdd.map(lambda l: l.asDict()).collect())

#store top ten crime records in x

x=counts.head(10)
x

#import libraries for plotting

import seaborn as sns
import matplotlib.pyplot as plt
import datetime
from pyspark.sql.functions import *

# Visualizing top ten crime types by count 

plt.rcParams["figure.figsize"] = [20, 8]

sns.set(style="whitegrid")

type_graph = sns.barplot(x='count', y='Primary_Type', data=x)
type_graph.set(ylabel="Type of crime", xlabel="Number of crimes")

"""**2. Visualize arrests over the years**"""

# Arrests, year, count through the years
arrest_type = df.groupBy(['Arrest', 'Year'])\
                     .count()\
                     .orderBy(['Year', 'count'], ascending=[True, False])
print()
arrest_type.show(3, truncate=False)

# A pandas data frame of above
arrest_type_p = pd.DataFrame(arrest_type.rdd.map(lambda l: l.asDict()).collect())

# Plot
t = arrest_type_p['count'] - 20 
s = arrest_type_p['Year']

#store arrested records and non-arrested records seperately
arrested = arrest_type_p[type_arrest_pddf['Arrest'] == 'TRUE']
not_arrested = arrest_type_p[type_arrest_pddf['Arrest'] == 'FALSE']

fig, ax = plt.subplots()
ax.plot(arrested['Year'], arrested['count'], label='Arrested')
ax.plot(not_arrested['Year'], not_arrested['count'], label='Not Arrested')

ax.set(xlabel='Years', ylabel='Total records',
       title='Arrests over the years')
ax.grid(b=True, which='both', axis='y')
ax.legend()

"""**3. Visualize number of crimes in each Location type (top 10)**"""

df.select('Location_Description').distinct().count()

#grouping locations and displaying count (ASC order)

Loc_desc = df.groupBy(['Location_Description']).count().orderBy('count', ascending=False)

#conver to pandas

LD = pd.DataFrame(Loc_desc.rdd.map(lambda l: l.asDict()).collect())

#ake top 10 records

LD = LD.head(10)

# Plot

plt.rcParams["figure.figsize"] = [30, 10]

sns.set(style="whitegrid")

type_graph = sns.barplot(x='Location_Description', y='count', data=LD)
type_graph.set(ylabel="Count", xlabel="Locations (Chicago)",title='Number of crimes location wise')

"""**4.Top 10 police stations (with District names) with domestic violence arrest rate**"""

#total count of domestic crimes
totCrimeDF = df.where(col("Domestic")==True).count()


print("Summary of the arrests which occurred for domestic violence for each Police Station")
df.where((col("Arrest")==True) & ((col("Domestic")==True)))\
           .select("District_Name","arrest","domestic")\
           .groupBy("District_Name")\
           .agg(count("District_Name").alias("Domestic Violence Arrest Rate"),\
                  (count("District_Name")/totCrimeDF*100).alias("Rate"))\
           .orderBy(col("Domestic Violence Arrest Rate").desc())\
           .select("District_Name","Domestic Violence Arrest Rate",round("Rate",2).alias("Ratio=Domestic Violence Cases/Reported Crimes")).show(10)

"""**Lets handle the Date column which is in String type**"""

#Convert to timestamp type
df = df.withColumn('date_time', to_timestamp('Date', 'MM/dd/yyyy hh:mm:ss a'))\
       .withColumn('date_short', trunc('date_time', 'YYYY'))

#check
df

# Take the 'hour' part of time into a new column called 'hour'
df_hour = df.withColumn('hour', hour(df['date_time']))

df_hour

#create a df with week, year and month columns
df_dates = df_hour.withColumn('week_day', dayofweek(df_hour['date_time']))\
                 .withColumn('year_month', month(df_hour['date_time']))\
                 .withColumn('month_day', dayofmonth(df_hour['date_time']))\
                 .withColumn('date_number', datediff(df['date_time'], to_date(lit('2001-01-01'), format='yyyy-MM-dd')))\
                 .cache()

#display all time columns 
df_dates.select(['date', 'date_short', 'hour', 'week_day', 'year', 'year_month', 'month_day', 'date_number']).show(20, truncate=False)

"""**5. Lets visualize which is the peak hour of the day for crimes in Chicago**"""

# Derive a data frame with crime counts per hour of the day:
count_by_hour = df_hour.groupBy(['Primary_Type', 'hour']).count().cache()
total_countbyhour = count_by_hour.groupBy('hour').sum('count')

count_by_hour_p = pd.DataFrame(total_countbyhour.select(total_countbyhour['hour'], total_countbyhour['sum(count)'].alias('count'))\
                                .rdd.map(lambda l: l.asDict())\
                                 .collect())

#sort by hours in ascending, to represent hours through the day
count_by_hour_p = count_by_hour_p.sort_values(by='hour')

count_by_hour_p

#plot 

plt.rcParams["figure.figsize"] = [20, 10]
fig, ax = plt.subplots()
ax.plot(count_by_hour_p['hour'], count_by_hour_p['count'], label='Hourly Count')

ax.set(xlabel='Hour of Day', ylabel='Number of crimes',
       title='Overall hourly crime numbers')
ax.grid(b=True, which='both', axis='y')
ax.legend()

"""**6. Let us visualize crimes by days of week, months of year and year**"""

crimes_by_weekday = df_dates.groupBy('week_day').count()

#convert to pandas dataframe

crimes_by_weekday_p = pd.DataFrame(week_day_crime_counts.orderBy('week_day').rdd.map(lambda e: e.asDict()).collect())

plt.rcParams["figure.figsize"] = [20, 10]

sns.set(style="whitegrid")

type_graph = sns.barplot(x='week_day', y='count', data=crimes_by_weekday_p)
type_graph.set(ylabel="Number of crimes", xlabel="Days of the week",title="Number of crimes by days of the week")

crime_by_month = df_dates.groupBy('year_month').count()

#convert to pandas
crime_by_month_p = pd.DataFrame(crime_by_month.orderBy('year_month').rdd.map(lambda e: e.asDict()).collect())

plt.rcParams["figure.figsize"] = [20, 10]

sns.set(style="whitegrid")
sns.set_color_codes("pastel")

type_graph = sns.barplot(x='year_month', y='count', data=year_month_crime_counts_pddf)
type_graph.set(ylabel="Number of crimes", xlabel="Months of the year",title="Number of crimes by months of year")

"""**PREDICTION MODEL**
Selected Predictors - 
'location_description'
'arrest'
'domestic'
'beat'
'district'
'ward'
'community_area'
'fbi_code'
'hour'
'week_day'
'year_month'
'month_day'
'date_number'


"""

#store our selected features 
features = [
 'Location_Description',
 'Arrest',
 'Domestic',
 'Beat',
 'District',
 'Ward',
 'Community_Area',
 'FBI_Code',
 'hour',
 'week_day',
 'year_month',
 'month_day',
 'date_number']

#Schema 
model_features = df_dates.select(features)
model_features.printSchema()

"""Indexing the string features from our selected list, to build a smooth model"""

#import required libraries
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import StringIndexer, VectorAssembler

df_dates_features = df_dates.na.drop(subset=features)

dic = []

for feature in features:
    print('Analysing %s' % feature)
    levels = model_features.select(feature).distinct()
    dic.append({'feature': feature, 'level_count': levels.count()})

for feature in dic:
    indexer = StringIndexer(inputCol=feature['feature'], outputCol='%s_indexed' % feature['feature'])
    print('Fitting feature "%s"' % feature['feature'])
    model = indexer.fit(df_dates_features)
    print('Transforming "%s"' % feature['feature'])
    df_dates_features = model.transform(df_dates_features)

## String-index the response variable:
response_indexer = StringIndexer(inputCol='Primary_Type', outputCol='indexed_primary_type')
response_model = response_indexer.fit(df_dates_features)
df_dates_features = response_model.transform(df_dates_features)

"""Vectorize the selected features"""

feature_indexed = ['%s_indexed' % fc['feature'] for fc in feature_level_count_dic]
feature_indexed

combiner = VectorAssembler(inputCols=feature_indexed, outputCol='features')
vectorized_df_dates = combiner.transform(df_dates_features)

vectorized_df_dates.select('features').take(1)

"""Split the dataset into train and test - 70% train data and 30% test data """

train, test = vectorized_df_dates.randomSplit([0.6, 0.4])

logistic_regression = LogisticRegression(labelCol='indexed_primary_type', featuresCol='features', maxIter=40, family='multinomial')

#fitting the train dataset onto a logistic regression model
fittedModel = logistic_regression.fit(train)

"""Performance of the model"""

#Model Accuracy
fittedModel.summary.accuracy

#Summary of the Model
Modelsummary = fittedModel.summary

fittedModel.coefficientMatrix

print(fittedModel.coefficientMatrix)

vectorized_df_dates.select('features').show(2, truncate=False)

label_stats = {float(i):{'index': float(i)} for i in range(34)}
print(label_stats)

#Calculating False Positive Rate, True Positive Rate, Precision, Recall ,F-measure for train data
print("False positive rate by label:")
for i, rate in enumerate(Modelsummary.falsePositiveRateByLabel):
    label_stats[i]['false_positive_rate'] = rate
    
for i, rate in enumerate(Modelsummary.truePositiveRateByLabel):
    label_stats[i]['true_positive_rate'] = rate
    
for i, rate in enumerate(Modelsummary.precisionByLabel):
    label_stats[i]['precision_rate'] = rate
    
for i, rate in enumerate(Modelsummary.recallByLabel):
    label_stats[i]['recall_rate'] = rate
    
for i, rate in enumerate(Modelsummary.fMeasureByLabel()):
    label_stats[i]['f_measure'] = rate

#Ordered By Indexed Primary Type
train_rdd = train.select(['primary_type', 'indexed_primary_type']).distinct().orderBy('indexed_primary_type').rdd.map(lambda l: l.asDict()).collect()

for l in train_rdd:
    print(l)
    label_stats[l['indexed_primary_type']]['primary_type'] = l['primary_type']

#Computing all the measures
rates_pddf = pd.DataFrame(list(label_stats.values()))

#Sorting the measures by highest precison rate to lowest
rates_pddf = rates_pddf.sort_values(by='precision_rate', ascending=False)

rates_pddf

"""**Test Data Performance**"""

#Fitting the test data set on the logistic regression model
fittedModel_test = logistic_regression.fit(test)

#Accuracy on Test Data
fittedModel_test.summary.accuracy

#Summary of the model
Modelsummary = fittedModel.summary

#Coefficient Matrix
fittedModel.coefficientMatrix

print(fittedModel.coefficientMatrix)

#secting the vectorized features
vectorized_df_dates.select('features').show(2, truncate=False)

label_stats = {float(i):{'index': float(i)} for i in range(34)}
print(label_stats)

#Calculating False Positive Rate, True Positive Rate, Precision, Recall ,F-measure for test data

print("False positive rate by label:")
for i, rate in enumerate(Modelsummary.falsePositiveRateByLabel):
    label_stats[i]['false_positive_rate'] = rate
    
for i, rate in enumerate(Modelsummary.truePositiveRateByLabel):
    label_stats[i]['true_positive_rate'] = rate
    
for i, rate in enumerate(Modelsummary.precisionByLabel):
    label_stats[i]['precision_rate'] = rate
    
for i, rate in enumerate(Modelsummary.recallByLabel):
    label_stats[i]['recall_rate'] = rate
    
for i, rate in enumerate(Modelsummary.fMeasureByLabel()):
    label_stats[i]['f_measure'] = rate

#Ordered By Indexed Primary Type
test_rdd = test.select(['primary_type','indexed_primary_type']).distinct().orderBy('indexed_primary_type').rdd.map(lambda l: l.asDict()).collect()

for l in test_rdd:
    print(l)
    label_stats[l['indexed_primary_type']]['primary_type'] = l['primary_type']

#Computing all the measures
rates_pddf = pd.DataFrame(list(label_stats.values()))

#Sorting the measures by highest precison rate to lowest.
rates_pddf = rates_pddf.sort_values(by='precision_rate', ascending=False)

rates_pddf